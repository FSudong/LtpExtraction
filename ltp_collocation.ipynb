{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于ltp挖掘评论观点\n",
    "\n",
    "“语言云” 以哈工大社会计算与信息检索研究中心研发的 “语言技术平台（LTP）” 为基础，为用户提供高效精准的中文自然语言处理云服务。 \n",
    "pyltp 是 LTP 的 Python 封装，提供了分词，词性标注，命名实体识别，依存句法分析，语义角色标注的功能。\n",
    "\n",
    "- 技术文档：http://pyltp.readthedocs.io/zh_CN/latest/api.html#id15 \n",
    "- 介绍文档：https://www.ltp-cloud.com/intro/#introduction \n",
    "- 介绍文档：http://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5\n",
    "\n",
    "需要先载入他们训练好的模型，(下载地址)[https://pan.baidu.com/share/link?shareid=1988562907&uk=2738088569#list/path=/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ltp模块\n",
    "import sys, os\n",
    "from pyltp import SentenceSplitter, Segmentor, Postagger, Parser, NamedEntityRecognizer, SementicRoleLabeller\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#segmentor.release()  # 释放模型\n",
    "\n",
    "class ltp_api(object):\n",
    "    def __init__(self,MODELDIR,exword_path = None):\n",
    "        self.MODELDIR = MODELDIR\n",
    "        self.output = {}\n",
    "        self.words = None\n",
    "        self.postags = None\n",
    "        self.netags = None\n",
    "        self.arcs = None\n",
    "        self.exword_path = exword_path  #  e.x: '/data1/research/matt/ltp/exwords.txt'\n",
    "        # 分词\n",
    "        self.segmentor = Segmentor()\n",
    "        if not self.exword_path:\n",
    "            # 是否加载额外词典\n",
    "            self.segmentor.load(os.path.join(self.MODELDIR, \"cws.model\"))\n",
    "        else:\n",
    "            self.segmentor.load_with_lexicon(os.path.join(self.MODELDIR, \"cws.model\"), self.exword_path)\n",
    "        \n",
    "        # 词性标注\n",
    "        self.postagger = Postagger()\n",
    "        self.postagger.load(os.path.join(self.MODELDIR, \"pos.model\"))\n",
    "        # 依存句法\n",
    "        self.parser = Parser()\n",
    "        self.parser.load(os.path.join(self.MODELDIR, \"parser.model\"))\n",
    "        # 命名实体识别\n",
    "        self.recognizer = NamedEntityRecognizer()\n",
    "        self.recognizer.load(os.path.join(self.MODELDIR, \"ner.model\"))\n",
    "        # 语义角色\n",
    "        self.labeller = SementicRoleLabeller()\n",
    "        self.labeller.load(os.path.join(MODELDIR, \"pisrl.model\"))\n",
    "        \n",
    "    # 分词\n",
    "    def ltp_segmentor(self,sentence):\n",
    "        words = self.segmentor.segment(sentence)\n",
    "        return words\n",
    "\n",
    "    # 词性标注\n",
    "    def ltp_postagger(self,words):\n",
    "        postags = self.postagger.postag(words)\n",
    "        return postags\n",
    "    \n",
    "    # 依存语法\n",
    "    def ltp_parser(self,words, postags):\n",
    "        arcs = self.parser.parse(words, postags)\n",
    "        return arcs\n",
    "    \n",
    "    # 命名实体识别\n",
    "    def ltp_recognizer(self,words, postags):\n",
    "        netags = self.recognizer.recognize(words, postags)\n",
    "        return netags\n",
    "    \n",
    "    # 语义角色识别\n",
    "    def ltp_labeller(self,words,postags, arcs):\n",
    "        output = []\n",
    "        roles = self.labeller.label(words, postags, arcs)\n",
    "        for role in roles:\n",
    "            output.append([(role.index,arg.name, arg.range.start, arg.range.end) for arg in role.arguments])\n",
    "        return output\n",
    "    \n",
    "    def release(self):\n",
    "        self.segmentor.release()\n",
    "        self.postagger.release()\n",
    "        self.parser.release()\n",
    "        self.recognizer.release()\n",
    "        self.labeller.release()\n",
    "        \n",
    "    def get_result(self,sentence):\n",
    "        self.words = self.ltp_segmentor(sentence)\n",
    "        self.postags = self.ltp_postagger(self.words)\n",
    "        self.arcs = self.ltp_parser(self.words, self.postags)\n",
    "        self.netags = self.ltp_recognizer(self.words, self.postags)\n",
    "        self.output['role'] = self.ltp_labeller(self.words,self.postags, self.arcs)\n",
    "    \n",
    "        # 载入output\n",
    "        self.output['words'] = list(self.words)\n",
    "        self.output['postags'] = list(self.postags)\n",
    "        self.output['arcs'] = [(arc.head, arc.relation) for arc in self.arcs]\n",
    "        self.output['netags'] = list(self.netags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析模块\n",
    "def get_tuples_word(word_list1,n1,word_list2,n2):\n",
    "    # 按照顺序，拼接词\n",
    "    result = []\n",
    "    for i,n1s,j,n2s in zip(word_list1,n1,word_list2,n2):\n",
    "        if n1s < n2s:\n",
    "            result.append(''.join([i,j])) \n",
    "        else :# n1s > n2s\n",
    "            result.append(''.join([j,i])) \n",
    "    return result\n",
    "\n",
    "def Parser2dataframe(words,postags,arcs):\n",
    "    '''\n",
    "    把依存句法解构成dataframe\n",
    "    '''\n",
    "    word_dict = dict(enumerate(words))\n",
    "    match_word = []\n",
    "    relation = []\n",
    "    pos = []\n",
    "    match_word_n = []\n",
    "    # 解读\n",
    "    for n,arc in enumerate(arcs):\n",
    "        relation_word = 'root ' if arc.head - 1 < 0 else word_dict[arc.head - 1]  # 核心词，root，为空\n",
    "        match_word.append(relation_word)\n",
    "        relation.append(arc.relation)\n",
    "        pos.append(postags[n])\n",
    "        match_word_n.append(0 if arc.head-1<0 else arc.head-1)\n",
    "        \n",
    "    tuples_words = pd.DataFrame({'word':list(word_dict.values()),'word_n':list(word_dict.keys()),\\\n",
    "                             'match_word':match_word,'relation':relation,'pos':pos,'match_word_n' : match_word_n})\n",
    "    tuples_words['tuples_words'] = get_tuples_word(tuples_words['word'],tuples_words['word_n'],\\\n",
    "                                                   tuples_words['match_word'],tuples_words['match_word_n'])\n",
    "    return tuples_words\n",
    "\n",
    "\n",
    "# 实体名词搭配\n",
    "def FindEntityCollocation(tuples_words,neg_words = ['是','又','而且','root']):\n",
    "    return [wo for wo in list(tuples_words['tuples_words'][tuples_words['pos']=='n']) if 'root' not in wo]\n",
    "\n",
    "# 通用内容搭配\n",
    "def FindCollocation(tuples_words,neg_words = ['是','又','而且']):\n",
    "    SBV_output,ADJ_output = '',''\n",
    "    if sum(tuples_words['relation']=='COO') > 0:\n",
    "        first_word = tuples_words['match_word'][tuples_words['relation']=='SBV']\n",
    "        second_word = tuples_words['word'][tuples_words['relation']=='SBV']\n",
    "        SBV_output = [wo for wo in list(zip(second_word,first_word)) if len(set(neg_words) & set(wo)) == 0 ]\n",
    "        \n",
    "    if (sum(tuples_words['relation']=='ADV')>0) or (sum(tuples_words['relation']=='ATT')>0):\n",
    "        # ADV部分\n",
    "        first_word = tuples_words['match_word'][tuples_words['relation']=='ADV']\n",
    "        second_word = tuples_words['word'][tuples_words['relation']=='ADV']\n",
    "        ADJ_output_1 = [wo for wo in list(zip(second_word,first_word)) if len(set(neg_words) & set(wo)) == 0 ]\n",
    "        # ATT部分\n",
    "        first_word = tuples_words['match_word'][tuples_words['relation']=='ATT']\n",
    "        second_word = tuples_words['word'][tuples_words['relation']=='ATT']\n",
    "        ADJ_output_2 = [wo for wo in list(zip(second_word,first_word)) if len(set(neg_words) & set(wo)) == 0 ]\n",
    "        # 相连\n",
    "        ADJ_output = ADJ_output_1 + ADJ_output_2\n",
    "    return SBV_output,ADJ_output\n",
    "\n",
    "# 并列名词查找\n",
    "def FindSynonym(tuples_words,neg_words = ['是','又','而且']):\n",
    "    output = ''\n",
    "    if sum(tuples_words['relation']=='COO') > 0:\n",
    "        first_word = tuples_words['match_word'][tuples_words['relation']=='COO']\n",
    "        second_word = tuples_words['word'][tuples_words['relation']=='COO']\n",
    "        output = [wo for wo in list(zip(second_word,first_word)) if len(set(neg_words) & set(wo)) == 0 ]\n",
    "    return output\n",
    "\n",
    "# 总结核心\n",
    "# 以：主 + 谓 + 宾为核心\n",
    "# sentense = '全书有数百个具体的例子，并被组织成了紧密的实用概念框架，能够适用于各个层次上的经理人与创业者。'\n",
    "def includeSth(sth,list_sth):\n",
    "    return [i in sth for i in list(list_sth)]\n",
    "\n",
    "def includeSBV_VOB(list_sth):\n",
    "    return True if sum([i in list(list_sth) for i in ['SBV','VOB']])==2 else False\n",
    "\n",
    "def SBV_VOB_bind(core_data,core_n,words):\n",
    "    SBV_VOB_n = list(core_data[includeSth(['SBV','VOB'],core_data['relation'])]['word_n'])\n",
    "    SBV_VOB_n.extend(list(core_n))\n",
    "    center_words = ''\n",
    "    for i in sorted(SBV_VOB_n):\n",
    "        center_words = ''.join([center_words,words[i]])\n",
    "    return center_words\n",
    "\n",
    "def CoreExtraction(tuples_words):\n",
    "    core_n = tuples_words[tuples_words['relation']=='HED']['word_n']\n",
    "    core_data = tuples_words[tuples_words['match_word_n']==int(core_n)]\n",
    "    core = ''\n",
    "    if includeSBV_VOB(core_data['relation']):\n",
    "        # SBV_VOB构成主谓宾，就是自动摘要了，最好两个都有\n",
    "        #print (SBV_VOB_bind(core_data,words))\n",
    "        core = SBV_VOB_bind(core_data,core_n,words)\n",
    "    elif sum(includeSth(['SBV'],core_data['relation']))>0:\n",
    "        # 主谓关系\n",
    "        #print (list(core_data[includeSth(['SBV'],core_data['relation'])]['tuples_words']))\n",
    "        core = list(core_data[includeSth(['SBV'],core_data['relation'])]['tuples_words'])\n",
    "    elif sum(includeSth(['VOB'],core_data['relation']))>0:\n",
    "        # 动宾关系\n",
    "        #print (list(core_data[includeSth(['VOB'],core_data['relation'])]['tuples_words']))\n",
    "        core = list(core_data[includeSth(['VOB'],core_data['relation'])]['tuples_words'])\n",
    "    elif sum(tuples_words['relation']=='HED')>0:\n",
    "        core = list(tuples_words['word'][tuples_words['relation']=='HED'])\n",
    "    return core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ltp初始化，但是有个问题就是一定需要释放，不能重复初始化，不然会出现爆内存的情况\n",
    "'''\n",
    "MODELDIR='ltp-models/ltp_data_v3.4.0'   #  模型文件\n",
    "ltp = ltp_api(MODELDIR)\n",
    "# ltp.release()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      "环境很好，位置独立性很强，比较安静很切合店名，半闲居，偷得半日闲。\n",
      "\n",
      "----- 搭配用语查找 -----\n",
      "\n",
      "([('环境', '好'), ('独立性', '强'), ('安静', '切合')], [('很', '好'), ('很', '强'), ('比较', '安静'), ('很', '切合'), ('位置', '独立性'), ('半', '日闲')])\n",
      "\n",
      "----- 并列词查找 -----\n",
      "\n",
      "[('强', '好'), ('切合', '好'), ('半闲居', '切合'), ('偷', '半闲居')]\n",
      "\n",
      "----- 核心观点抽取 -----\n",
      "\n",
      "['环境好']\n",
      "\n",
      "----- 实体名词搭配 -----\n",
      "\n",
      "['环境好', '位置独立性', '独立性强', '切合店名']\n",
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      "点了比较经典的菜品，味道果然不错！\n",
      "\n",
      "----- 搭配用语查找 -----\n",
      "\n",
      "([('味道', '不错')], [('比较', '经典'), ('果然', '不错'), ('经典', '菜品')])\n",
      "\n",
      "----- 并列词查找 -----\n",
      "\n",
      "[('不错', '点')]\n",
      "\n",
      "----- 核心观点抽取 -----\n",
      "\n",
      "['点菜品']\n",
      "\n",
      "----- 实体名词搭配 -----\n",
      "\n",
      "['点菜品', '味道不错']\n",
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      "烤乳鸽，\n",
      "\n",
      "----- 搭配用语查找 -----\n",
      "\n",
      "('', '')\n",
      "\n",
      "----- 并列词查找 -----\n",
      "\n",
      "\n",
      "\n",
      "----- 核心观点抽取 -----\n",
      "\n",
      "['烤乳鸽']\n",
      "\n",
      "----- 实体名词搭配 -----\n",
      "\n",
      "[]\n",
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      "超级赞赞赞，脆皮焦香，肉质细嫩，超好吃。\n",
      "\n",
      "----- 搭配用语查找 -----\n",
      "\n",
      "([('肉质', '细嫩')], [('超级', '赞赞赞'), ('超', '好吃'), ('脆皮', '焦香')])\n",
      "\n",
      "----- 并列词查找 -----\n",
      "\n",
      "[('焦香', '赞赞赞'), ('细嫩', '焦香'), ('好吃', '细嫩')]\n",
      "\n",
      "----- 核心观点抽取 -----\n",
      "\n",
      "['赞赞赞']\n",
      "\n",
      "----- 实体名词搭配 -----\n",
      "\n",
      "['脆皮焦香', '赞赞赞焦香', '肉质细嫩']\n",
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      "艇仔粥料很足，香葱自己添加，很贴心。\n",
      "\n",
      "----- 搭配用语查找 -----\n",
      "\n",
      "([('艇仔粥料', '足'), ('自己', '添加')], [('很', '足'), ('很', '贴心'), ('香葱', '自己')])\n",
      "\n",
      "----- 并列词查找 -----\n",
      "\n",
      "[('添加', '足'), ('贴心', '添加')]\n",
      "\n",
      "----- 核心观点抽取 -----\n",
      "\n",
      "['艇仔粥料足']\n",
      "\n",
      "----- 实体名词搭配 -----\n",
      "\n",
      "['艇仔粥料足', '香葱自己']\n",
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      "金钱肚味道不错，不过没有在广州吃的烂，牙口不好的慎点。\n",
      "\n",
      "----- 搭配用语查找 -----\n",
      "\n",
      "([('味道', '不错'), ('牙口', '不好')], [('不过', '吃'), ('没有', '吃'), ('在', '吃'), ('金钱', '肚'), ('肚', '味道'), ('不好', '慎点')])\n",
      "\n",
      "----- 并列词查找 -----\n",
      "\n",
      "[('吃', '不错'), ('慎点', '吃')]\n",
      "\n",
      "----- 核心观点抽取 -----\n",
      "\n",
      "['味道不错']\n",
      "\n",
      "----- 实体名词搭配 -----\n",
      "\n",
      "['金钱肚', '肚味道', '味道不错', '牙口不好', '吃慎点']\n",
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      "凤爪很火候很好，推荐。\n",
      "\n",
      "----- 搭配用语查找 -----\n",
      "\n",
      "([('凤爪', '火候')], [('很', '火候'), ('很', '好')])\n",
      "\n",
      "----- 并列词查找 -----\n",
      "\n",
      "[('好', '火候'), ('推荐', '火候')]\n",
      "\n",
      "----- 核心观点抽取 -----\n",
      "\n",
      "['凤爪火候']\n",
      "\n",
      "----- 实体名词搭配 -----\n",
      "\n",
      "['凤爪火候']\n",
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      "最惊艳的是长寿菜，菜料十足，很新鲜，清淡又不乏味道，而且没有添加调料的味道，搭配的非常不错！\n",
      "\n",
      "----- 搭配用语查找 -----\n",
      "\n",
      "([('菜料', '十足'), ('搭配', '不错')], [('最', '惊艳'), ('很', '新鲜'), ('非常', '不错'), ('添加', '味道')])\n",
      "\n",
      "----- 并列词查找 -----\n",
      "\n",
      "[('清淡', '新鲜'), ('不乏', '清淡'), ('没有', '新鲜'), ('不错', '没有')]\n",
      "\n",
      "----- 核心观点抽取 -----\n",
      "\n",
      "惊艳是长寿菜\n",
      "\n",
      "----- 实体名词搭配 -----\n",
      "\n",
      "['是长寿菜', '菜料十足', '不乏味道', '添加调料', '没有味道']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"环境很好，位置独立性很强，比较安静很切合店名，半闲居，偷得半日闲。点了比较经典的菜品，味道果然不错！烤乳鸽，\n",
    "超级赞赞赞，脆皮焦香，肉质细嫩，超好吃。艇仔粥料很足，香葱自己添加，很贴心。金钱肚味道不错，不过没有在广州吃的烂，牙口不好的慎点。\n",
    "凤爪很火候很好，推荐。最惊艳的是长寿菜，菜料十足，很新鲜，清淡又不乏味道，而且没有添加调料的味道，搭配的非常不错！\"\"\"\n",
    "sentences = SentenceSplitter.split(paragraph)\n",
    "\n",
    "for sentence in sentences:\n",
    "    if sentence:\n",
    "        print('\\n===================== 原句 =====================\\n')\n",
    "        print(sentence)\n",
    "        # 第一种：类内计算\n",
    "        words = ltp.ltp_segmentor(sentence)  # 分词\n",
    "        postags = ltp.ltp_postagger(words)  # 词性\n",
    "        arcs = ltp.ltp_parser(words,postags)  #依存\n",
    "        netags = ltp.ltp_recognizer(words,postags)# 命名实体识别\n",
    "        labeller = ltp.ltp_labeller(words,postags, arcs) #语义角色\n",
    "        \n",
    "        #ltp.get_result(sentence)\n",
    "        #output = ltp.output\n",
    "        #arcs = output['arcs']\n",
    "        #netags = output['netags']\n",
    "        #postags = output['postags']\n",
    "        #labeller = output['role']\n",
    "        #words = output['words']\n",
    "        tuples_words = Parser2dataframe(words,postags,arcs)\n",
    "\n",
    "        print('\\n----- 搭配用语查找 -----\\n')\n",
    "        print(FindCollocation(tuples_words))\n",
    "        print('\\n----- 并列词查找 -----\\n')\n",
    "        print(FindSynonym(tuples_words))\n",
    "        print('\\n----- 核心观点抽取 -----\\n')\n",
    "        print(CoreExtraction(tuples_words))\n",
    "        print('\\n----- 实体名词搭配 -----\\n')\n",
    "        print(FindEntityCollocation(tuples_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
