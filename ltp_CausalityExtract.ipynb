{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三元组与因果推理\n",
    "\n",
    "\n",
    "[liuhuanyong/EventTriplesExtraction](https://github.com/liuhuanyong/EventTriplesExtraction)\n",
    "\n",
    "基于依存句法与语义角色标注的事件三元组抽取 文本表示一直是个重要问题，如何以清晰，简介的方式对一个文本信息进行有效表示是个长远的任务\n",
    "\n",
    "我尝试过使用关键词，实体之间的关联关系，并使用textgrapher的方式进行展示，但以词作为文本信息单元表示这种效果不是特别好，所以，本项目想尝试从事件三元组的方式出发，对文本进行表示．\n",
    "\n",
    "同一位作者还公开了他的因果推理：https://github.com/liuhuanyong/CausalityEventExtraction/blob/master/causality_extract.py\n",
    "\n",
    "笔者这边觉得除了以上两项，还需要有：\n",
    "\n",
    "- 事件抽取\n",
    "- 评论观点抽取\n",
    "- 因果关系抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyltp import Segmentor, Postagger, Parser, NamedEntityRecognizer, SementicRoleLabeller\n",
    "import re\n",
    "class LtpParser:\n",
    "    def __init__(self):\n",
    "        LTP_DIR = \"ltp-models/ltp_data_v3.4.0\"\n",
    "        self.segmentor = Segmentor()\n",
    "        self.segmentor.load(os.path.join(LTP_DIR, \"cws.model\"))\n",
    "\n",
    "        self.postagger = Postagger()\n",
    "        self.postagger.load(os.path.join(LTP_DIR, \"pos.model\"))\n",
    "\n",
    "        self.parser = Parser()\n",
    "        self.parser.load(os.path.join(LTP_DIR, \"parser.model\"))\n",
    "\n",
    "        self.recognizer = NamedEntityRecognizer()\n",
    "        self.recognizer.load(os.path.join(LTP_DIR, \"ner.model\"))\n",
    "\n",
    "        self.labeller = SementicRoleLabeller()\n",
    "        self.labeller.load(os.path.join(LTP_DIR, 'pisrl.model'))\n",
    "\n",
    "    '''语义角色标注'''\n",
    "    def format_labelrole(self, words, postags):\n",
    "        arcs = self.parser.parse(words, postags)\n",
    "        roles = self.labeller.label(words, postags, arcs)\n",
    "        roles_dict = {}\n",
    "        for role in roles:\n",
    "            roles_dict[role.index] = {arg.name:[arg.name,arg.range.start, arg.range.end] for arg in role.arguments}\n",
    "        return roles_dict\n",
    "\n",
    "    '''句法分析---为句子中的每个词语维护一个保存句法依存儿子节点的字典'''\n",
    "    def build_parse_child_dict(self, words, postags, arcs):\n",
    "        child_dict_list = []\n",
    "        format_parse_list = []\n",
    "        for index in range(len(words)):\n",
    "            child_dict = dict()\n",
    "            for arc_index in range(len(arcs)):\n",
    "                if arcs[arc_index].head == index+1:   #arcs的索引从1开始\n",
    "                    if arcs[arc_index].relation in child_dict:\n",
    "                        child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                    else:\n",
    "                        child_dict[arcs[arc_index].relation] = []\n",
    "                        child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "            child_dict_list.append(child_dict)\n",
    "        rely_id = [arc.head for arc in arcs]  # 提取依存父节点id\n",
    "        relation = [arc.relation for arc in arcs]  # 提取依存关系\n",
    "        heads = ['Root' if id == 0 else words[id - 1] for id in rely_id]  # 匹配依存父节点词语\n",
    "        for i in range(len(words)):\n",
    "            # ['ATT', '李克强', 0, 'nh', '总理', 1, 'n']\n",
    "            a = [relation[i], words[i], i, postags[i], heads[i], rely_id[i]-1, postags[rely_id[i]-1]]\n",
    "            format_parse_list.append(a)\n",
    "\n",
    "        return child_dict_list, format_parse_list\n",
    "\n",
    "    '''parser主函数'''\n",
    "    def parser_main(self, sentence):\n",
    "        words = list(self.segmentor.segment(sentence))\n",
    "        postags = list(self.postagger.postag(words))\n",
    "        arcs = self.parser.parse(words, postags)\n",
    "        child_dict_list, format_parse_list = self.build_parse_child_dict(words, postags, arcs)\n",
    "        roles_dict = self.format_labelrole(words, postags)\n",
    "        return words, postags, child_dict_list, roles_dict, format_parse_list\n",
    "\n",
    "\n",
    "class TripleExtractor:\n",
    "    def __init__(self):\n",
    "        self.parser = LtpParser()\n",
    "\n",
    "    '''文章分句处理, 切分长句，冒号，分号，感叹号等做切分标识'''\n",
    "    def split_sents(self, content):\n",
    "        return [sentence for sentence in re.split(r'[？?！!。；;：:\\n\\r]', content) if sentence]\n",
    "\n",
    "    '''利用语义角色标注,直接获取主谓宾三元组,基于A0,A1,A2'''\n",
    "    def ruler1(self, words, postags, roles_dict, role_index):\n",
    "        v = words[role_index]\n",
    "        role_info = roles_dict[role_index]\n",
    "        if 'A0' in role_info.keys() and 'A1' in role_info.keys():\n",
    "            s = ''.join([words[word_index] for word_index in range(role_info['A0'][1], role_info['A0'][2]+1) if\n",
    "                         postags[word_index][0] not in ['w', 'u', 'x'] and words[word_index]])\n",
    "            o = ''.join([words[word_index] for word_index in range(role_info['A1'][1], role_info['A1'][2]+1) if\n",
    "                         postags[word_index][0] not in ['w', 'u', 'x'] and words[word_index]])\n",
    "            if s  and o:\n",
    "                return '1', [s, v, o]\n",
    "        # elif 'A0' in role_info:\n",
    "        #     s = ''.join([words[word_index] for word_index in range(role_info['A0'][1], role_info['A0'][2] + 1) if\n",
    "        #                  postags[word_index][0] not in ['w', 'u', 'x']])\n",
    "        #     if s:\n",
    "        #         return '2', [s, v]\n",
    "        # elif 'A1' in role_info:\n",
    "        #     o = ''.join([words[word_index] for word_index in range(role_info['A1'][1], role_info['A1'][2]+1) if\n",
    "        #                  postags[word_index][0] not in ['w', 'u', 'x']])\n",
    "        #     return '3', [v, o]\n",
    "        return '4', []\n",
    "\n",
    "    '''三元组抽取主函数'''\n",
    "    def ruler2(self, words, postags, child_dict_list, arcs, roles_dict):\n",
    "        svos = []\n",
    "        for index in range(len(postags)):\n",
    "            tmp = 1\n",
    "            # 先借助语义角色标注的结果，进行三元组抽取\n",
    "            if index in roles_dict:\n",
    "                flag, triple = self.ruler1(words, postags, roles_dict, index)\n",
    "                if flag == '1':\n",
    "                    svos.append(triple)\n",
    "                    tmp = 0\n",
    "            if tmp == 1:\n",
    "                # 如果语义角色标记为空，则使用依存句法进行抽取\n",
    "                # if postags[index] == 'v':\n",
    "                if postags[index]:\n",
    "                # 抽取以谓词为中心的事实三元组\n",
    "                    child_dict = child_dict_list[index]\n",
    "                    # 主谓宾\n",
    "                    if 'SBV' in child_dict and 'VOB' in child_dict:\n",
    "                        r = words[index]\n",
    "                        e1 = self.complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                        e2 = self.complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                        svos.append([e1, r, e2])\n",
    "\n",
    "                    # 定语后置，动宾关系\n",
    "                    relation = arcs[index][0]\n",
    "                    head = arcs[index][2]\n",
    "                    if relation == 'ATT':\n",
    "                        if 'VOB' in child_dict:\n",
    "                            e1 = self.complete_e(words, postags, child_dict_list, head - 1)\n",
    "                            r = words[index]\n",
    "                            e2 = self.complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                            temp_string = r + e2\n",
    "                            if temp_string == e1[:len(temp_string)]:\n",
    "                                e1 = e1[len(temp_string):]\n",
    "                            if temp_string not in e1:\n",
    "                                svos.append([e1, r, e2])\n",
    "                    # 含有介宾关系的主谓动补关系\n",
    "                    if 'SBV' in child_dict and 'CMP' in child_dict:\n",
    "                        e1 = self.complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                        cmp_index = child_dict['CMP'][0]\n",
    "                        r = words[index] + words[cmp_index]\n",
    "                        if 'POB' in child_dict_list[cmp_index]:\n",
    "                            e2 = self.complete_e(words, postags, child_dict_list, child_dict_list[cmp_index]['POB'][0])\n",
    "                            svos.append([e1, r, e2])\n",
    "        return svos\n",
    "\n",
    "    '''对找出的主语或者宾语进行扩展'''\n",
    "    def complete_e(self, words, postags, child_dict_list, word_index):\n",
    "        child_dict = child_dict_list[word_index]\n",
    "        prefix = ''\n",
    "        if 'ATT' in child_dict:\n",
    "            for i in range(len(child_dict['ATT'])):\n",
    "                prefix += self.complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "        postfix = ''\n",
    "        if postags[word_index] == 'v':\n",
    "            if 'VOB' in child_dict:\n",
    "                postfix += self.complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "            if 'SBV' in child_dict:\n",
    "                prefix = self.complete_e(words, postags, child_dict_list, child_dict['SBV'][0]) + prefix\n",
    "\n",
    "        return prefix + words[word_index] + postfix\n",
    "\n",
    "    '''程序主控函数'''\n",
    "    def triples_main(self, content):\n",
    "        sentences = self.split_sents(content)\n",
    "        svos = []\n",
    "        for sentence in sentences:\n",
    "            words, postags, child_dict_list, roles_dict, arcs = self.parser.parser_main(sentence)\n",
    "            svo = self.ruler2(words, postags, child_dict_list, arcs, roles_dict)\n",
    "            svos += svo\n",
    "\n",
    "        return svos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TripleExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================== 原句 =====================\n",
      "\n",
      " 以色列国防军20日对加沙地带实施轰炸，造成3名巴勒斯坦武装人员死亡。此外，巴勒斯坦人与以色列士兵当天在加沙地带与以交界地区发生冲突，一名巴勒斯坦人被打死。当天的冲突还造成210名巴勒斯坦人受伤。\n",
      "当天，数千名巴勒斯坦人在加沙地带边境地区继续“回归大游行”抗议活动。部分示威者燃烧轮胎，并向以军投掷石块、燃烧瓶等，驻守边境的以军士兵向示威人群发射催泪瓦斯并开枪射击。\n",
      "\n",
      "----- 三元组 -----\n",
      "\n",
      "三元组: [['以色列国防军', '实施', '轰炸'], ['冲突', '发生', '巴勒斯坦人与以色列士兵'], ['当天冲突', '造成', '受伤'], ['数千名巴勒斯坦人', '继续', '回归大游行抗议活动'], ['“', '回归', '大游行'], ['部分示威者', '投掷', '石块'], ['，', '驻守', '边境'], ['驻守边境以军士兵', '发射', '催泪瓦斯']]\n"
     ]
    }
   ],
   "source": [
    "content1 = \"\"\"环境很好，位置独立性很强，比较安静很切合店名，半闲居，偷得半日闲。点了比较经典的菜品，味道果然不错！烤乳鸽，超级赞赞赞，脆皮焦香，肉质细嫩，超好吃。艇仔粥料很足，香葱自己添加，很贴心。金钱肚味道不错，不过没有在广州吃的烂，牙口不好的慎点。凤爪很火候很好，推荐。最惊艳的是长寿菜，菜料十足，很新鲜，清淡又不乏味道，而且没有添加调料的味道，搭配的非常不错！\"\"\"\n",
    "content2 = \"\"\"近日，一条男子高铁吃泡面被女乘客怒怼的视频引发热议。女子情绪激动，言辞激烈，大声斥责该乘客，称高铁上有规定不能吃泡面，质问其“有公德心吗”“没素质”。视频曝光后，该女子回应称，因自己的孩子对泡面过敏，曾跟这名男子沟通过，但对方执意不听，她才发泄不满，并称男子拍视频上传已侵犯了她的隐私权和名誉权，将采取法律手段。12306客服人员表示，高铁、动车上一般不卖泡面，但没有规定高铁、动车上不能吃泡面。\n",
    "            高铁属于密封性较强的空间，每名乘客都有维护高铁内秩序，不破坏该空间内空气质量的义务。这也是乘客作为公民应当具备的基本品质。但是，在高铁没有明确禁止食用泡面等食物的背景下，以影响自己或孩子为由阻挠他人食用某种食品并厉声斥责，恐怕也超出了权利边界。当人们在公共场所活动时，不宜过分干涉他人权利，这样才能构建和谐美好的公共秩序。\n",
    "            一般来说，个人的权利便是他人的义务，任何人不得随意侵犯他人权利，这是每个公民得以正常工作、生活的基本条件。如果权利可以被肆意侵犯而得不到救济，社会将无法运转，人们也没有幸福可言。如西谚所说，“你的权利止于我的鼻尖”，“你可以唱歌，但不能在午夜破坏我的美梦”。无论何种权利，其能够得以行使的前提是不影响他人正常生活，不违反公共利益和公序良俗。超越了这个边界，权利便不再为权利，也就不再受到保护。\n",
    "            在“男子高铁吃泡面被怒怼”事件中，初一看，吃泡面男子可能侵犯公共场所秩序，被怒怼乃咎由自取，其实不尽然。虽然高铁属于封闭空间，但与禁止食用刺激性食品的地铁不同，高铁运营方虽然不建议食用泡面等刺激性食品，但并未作出禁止性规定。由此可见，即使食用泡面、榴莲、麻辣烫等食物可能产生刺激性味道，让他人不适，但是否食用该食品，依然取决于个人喜好，他人无权随意干涉乃至横加斥责。这也是此事件披露后，很多网友并未一边倒地批评食用泡面的男子，反而认为女乘客不该高声喧哗。\n",
    "            现代社会，公民的义务一般分为法律义务和道德义务。如果某个行为被确定为法律义务，行为人必须遵守，一旦违反，无论是受害人抑或旁观群众，均有权制止、投诉、举报。违法者既会受到应有惩戒，也会受到道德谴责，积极制止者则属于应受鼓励的见义勇为。如果有人违反道德义务，则应受到道德和舆论谴责，并有可能被追究法律责任。如在公共场所随地吐痰、乱扔垃圾、脱掉鞋子、随意插队等。此时，如果行为人对他人的劝阻置之不理甚至行凶报复，无疑要受到严厉惩戒。\n",
    "            当然，随着社会的发展，某些道德义务可能上升为法律义务。如之前，很多人对公共场所吸烟不以为然，烟民可以旁若无人地吞云吐雾。现在，要是还有人不识时务地在公共场所吸烟，必然将成为众矢之的。\n",
    "            再回到“高铁吃泡面”事件，要是随着人们观念的更新，在高铁上不得吃泡面等可能产生刺激性气味的食物逐渐成为共识，或者上升到道德义务或法律义务。斥责、制止他人吃泡面将理直气壮，否则很难摆脱“矫情”，“将自我权利凌驾于他人权利之上”的嫌疑。\n",
    "            在相关部门并未禁止在高铁上吃泡面的背景下，吃不吃泡面系个人权利或者个人私德，是不违反公共利益的个人正常生活的一部分。如果认为他人吃泡面让自己不适，最好是请求他人配合并加以感谢，而非站在道德制高点强制干预。只有每个人行使权利时不逾越边界，与他人沟通时好好说话，不过分自我地将幸福和舒适凌驾于他人之上，人与人之间才更趋于平等，公共生活才更趋向美好有序。\"\"\"\n",
    "content3 = '''（原标题：央视独家采访：陕西榆林产妇坠楼事件在场人员还原事情经过）\n",
    "央视新闻客户端11月24日消息，2017年8月31日晚，在陕西省榆林市第一医院绥德院区，产妇马茸茸在待产时，从医院五楼坠亡。事发后，医院方面表示，由于家属多次拒绝剖宫产，最终导致产妇难忍疼痛跳楼。但是产妇家属却声称，曾向医生多次提出剖宫产被拒绝。\n",
    "事情经过究竟如何，曾引起舆论纷纷，而随着时间的推移，更多的反思也留给了我们，只有解决了这起事件中暴露出的一些问题，比如患者的医疗选择权，人们对剖宫产和顺产的认识问题等，这样的悲剧才不会再次发生。央视记者找到了等待产妇的家属，主治医生，病区主任，以及当时的两位助产师，一位实习医生，希望通过他们的讲述，更准确地还原事情经过。\n",
    "产妇待产时坠亡，事件有何疑点。公安机关经过调查，排除他杀可能，初步认定马茸茸为跳楼自杀身亡。马茸茸为何会在医院待产期间跳楼身亡，这让所有人的目光都聚焦到了榆林第一医院，这家在当地人心目中数一数二的大医院。\n",
    "就这起事件来说，如何保障患者和家属的知情权，如何让患者和医生能够多一份实质化的沟通？这就需要与之相关的法律法规更加的细化、人性化并且充满温度。用这种温度来消除孕妇对未知的恐惧，来保障医患双方的权益，迎接新生儿平安健康地来到这个世界。'''\n",
    "content4 = '李克强总理今天来我家了,我感到非常荣幸'\n",
    "content5 = ''' 以色列国防军20日对加沙地带实施轰炸，造成3名巴勒斯坦武装人员死亡。此外，巴勒斯坦人与以色列士兵当天在加沙地带与以交界地区发生冲突，一名巴勒斯坦人被打死。当天的冲突还造成210名巴勒斯坦人受伤。\n",
    "当天，数千名巴勒斯坦人在加沙地带边境地区继续“回归大游行”抗议活动。部分示威者燃烧轮胎，并向以军投掷石块、燃烧瓶等，驻守边境的以军士兵向示威人群发射催泪瓦斯并开枪射击。'''\n",
    "\n",
    "print('\\n===================== 原句 =====================\\n')\n",
    "print(content5)\n",
    "svos = extractor.triples_main(content5)\n",
    "print('\\n----- 三元组 -----\\n')\n",
    "print('三元组:', svos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 因果推理挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, jieba\n",
    "import jieba.posseg as pseg\n",
    "from pyltp import SentenceSplitter\n",
    "class CausalityExractor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    '''1由果溯因配套式'''\n",
    "    def ruler1(self, sentence):\n",
    "        '''\n",
    "        conm2:〈[之]所以,因为〉、〈[之]所以,由于〉、 <[之]所以,缘于〉\n",
    "        conm2_model:<Conj>{Effect},<Conj>{Cause}\n",
    "        '''\n",
    "        datas = list()\n",
    "        word_pairs =[['之?所以', '因为'], ['之?所以', '由于'], ['之?所以', '缘于']]\n",
    "        for word in word_pairs:\n",
    "            pattern = re.compile(r'\\s?(%s)/[p|c]+\\s(.*)(%s)/[p|c]+\\s(.*)' % (word[0], word[1]))\n",
    "            result = pattern.findall(sentence)\n",
    "            data = dict()\n",
    "            if result:\n",
    "                data['tag'] = result[0][0] + '-' + result[0][2]\n",
    "                data['cause'] = result[0][3]\n",
    "                data['effect'] = result[0][1]\n",
    "                datas.append(data)\n",
    "        if datas:\n",
    "            return datas[0]\n",
    "        else:\n",
    "            return {}\n",
    "    '''2由因到果配套式'''\n",
    "    def ruler2(self, sentence):\n",
    "        '''\n",
    "        conm1:〈因为,从而〉、〈因为,为此〉、〈既[然],所以〉、〈因为,为此〉、〈由于,为此〉、〈只有|除非,才〉、〈由于,以至[于]>、〈既[然],却>、\n",
    "        〈如果,那么|则〉、<由于,从而〉、<既[然],就〉、〈既[然],因此〉、〈如果,就〉、〈只要,就〉〈因为,所以〉、 <由于,于是〉、〈因为,因此〉、\n",
    "         <由于,故〉、 〈因为,以致[于]〉、〈因为,因而〉、〈由于,因此〉、<因为,于是〉、〈由于,致使〉、〈因为,致使〉、〈由于,以致[于] >\n",
    "         〈因为,故〉、〈因[为],以至[于]>,〈由于,所以〉、〈因为,故而〉、〈由于,因而〉\n",
    "        conm1_model:<Conj>{Cause}, <Conj>{Effect}\n",
    "        '''\n",
    "        datas = list()\n",
    "        word_pairs =[['因为', '从而'], ['因为', '为此'], ['既然?', '所以'],\n",
    "                    ['因为', '为此'], ['由于', '为此'], ['除非', '才'],\n",
    "                    ['只有', '才'], ['由于', '以至于?'], ['既然?', '却'],\n",
    "                    ['如果', '那么'], ['如果', '则'], ['由于', '从而'],\n",
    "                    ['既然?', '就'], ['既然?', '因此'], ['如果', '就'],\n",
    "                    ['只要', '就'], ['因为', '所以'], ['由于', '于是'],\n",
    "                    ['因为', '因此'], ['由于', '故'], ['因为', '以致于?'],\n",
    "                    ['因为', '以致'], ['因为', '因而'], ['由于', '因此'],\n",
    "                    ['因为', '于是'], ['由于', '致使'], ['因为', '致使'],\n",
    "                    ['由于', '以致于?'], ['因为', '故'], ['因为?', '以至于?'],\n",
    "                    ['由于', '所以'], ['因为', '故而'], ['由于', '因而']]\n",
    "\n",
    "        for word in word_pairs:\n",
    "            pattern = re.compile(r'\\s?(%s)/[p|c]+\\s(.*)(%s)/[p|c]+\\s(.*)' % (word[0], word[1]))\n",
    "            result = pattern.findall(sentence)\n",
    "            data = dict()\n",
    "            if result:\n",
    "                data['tag'] = result[0][0] + '-' + result[0][2]\n",
    "                data['cause'] = result[0][1]\n",
    "                data['effect'] = result[0][3]\n",
    "                datas.append(data)\n",
    "        if datas:\n",
    "            return datas[0]\n",
    "        else:\n",
    "            return {}\n",
    "    '''3由因到果居中式明确'''\n",
    "    def ruler3(self, sentence):\n",
    "        '''\n",
    "        cons2:于是、所以、故、致使、以致[于]、因此、以至[于]、从而、因而\n",
    "        cons2_model:{Cause},<Conj...>{Effect}\n",
    "        '''\n",
    "\n",
    "        pattern = re.compile(r'(.*)[,，]+.*(于是|所以|故|致使|以致于?|因此|以至于?|从而|因而)/[p|c]+\\s(.*)')\n",
    "        result = pattern.findall(sentence)\n",
    "        data = dict()\n",
    "        if result:\n",
    "            data['tag'] = result[0][1]\n",
    "            data['cause'] = result[0][0]\n",
    "            data['effect'] = result[0][2]\n",
    "        return data\n",
    "    '''4由因到果居中式精确'''\n",
    "    def ruler4(self, sentence):\n",
    "        '''\n",
    "        verb1:牵动、导向、使动、导致、勾起、引入、指引、使、予以、产生、促成、造成、引导、造就、促使、酿成、\n",
    "            引发、渗透、促进、引起、诱导、引来、促发、引致、诱发、推进、诱致、推动、招致、影响、致使、滋生、归于、\n",
    "            作用、使得、决定、攸关、令人、引出、浸染、带来、挟带、触发、关系、渗入、诱惑、波及、诱使\n",
    "        verb1_model:{Cause},<Verb|Adverb...>{Effect}\n",
    "        '''\n",
    "        pattern = re.compile(r'(.*)\\s+(牵动|已致|导向|使动|导致|勾起|引入|指引|使|予以|产生|促成|造成|引导|造就|促使|酿成|引发|渗透|促进|引起|诱导|引来|促发|引致|诱发|推进|诱致|推动|招致|影响|致使|滋生|归于|作用|使得|决定|攸关|令人|引出|浸染|带来|挟带|触发|关系|渗入|诱惑|波及|诱使)/[d|v]+\\s(.*)')\n",
    "        result = pattern.findall(sentence)\n",
    "        data = dict()\n",
    "        if result:\n",
    "            data['tag'] = result[0][1]\n",
    "            data['cause'] = result[0][0]\n",
    "            data['effect'] = result[0][2]\n",
    "        return data\n",
    "    '''5由因到果前端式模糊'''\n",
    "    def ruler5(self, sentence):\n",
    "        '''\n",
    "        prep:为了、依据、为、按照、因[为]、按、依赖、照、比、凭借、由于\n",
    "        prep_model:<Prep...>{Cause},{Effect}\n",
    "        '''\n",
    "        pattern = re.compile(r'\\s?(为了|依据|按照|因为|因|按|依赖|凭借|由于)/[p|c]+\\s(.*)[,，]+(.*)')\n",
    "        result = pattern.findall(sentence)\n",
    "        data = dict()\n",
    "        if result:\n",
    "            data['tag'] = result[0][0]\n",
    "            data['cause'] = result[0][1]\n",
    "            data['effect'] = result[0][2]\n",
    "\n",
    "        return data\n",
    "\n",
    "    '''6由因到果居中式模糊'''\n",
    "    def ruler6(self, sentence):\n",
    "        '''\n",
    "        adverb:以免、以便、为此、才\n",
    "        adverb_model:{Cause},<Verb|Adverb...>{Effect}\n",
    "        '''\n",
    "        pattern = re.compile(r'(.*)(以免|以便|为此|才)\\s(.*)')\n",
    "        result = pattern.findall(sentence)\n",
    "        data = dict()\n",
    "        if result:\n",
    "            data['tag'] = result[0][1]\n",
    "            data['cause'] = result[0][0]\n",
    "            data['effect'] = result[0][2]\n",
    "        return data\n",
    "\n",
    "    '''7由因到果前端式精确'''\n",
    "    def ruler7(self, sentence):\n",
    "        '''\n",
    "        cons1:既[然]、因[为]、如果、由于、只要\n",
    "        cons1_model:<Conj...>{Cause},{Effect}\n",
    "        '''\n",
    "        pattern = re.compile(r'\\s?(既然?|因|因为|如果|由于|只要)/[p|c]+\\s(.*)[,，]+(.*)')\n",
    "        result = pattern.findall(sentence)\n",
    "        data = dict()\n",
    "        if result:\n",
    "            data['tag'] = result[0][0]\n",
    "            data['cause'] = result[0][1]\n",
    "            data['effect'] = result[0][2]\n",
    "        return data\n",
    "    '''8由果溯因居中式模糊'''\n",
    "    def ruler8(self, sentence):\n",
    "        '''\n",
    "        3\n",
    "        verb2:根源于、取决、来源于、出于、取决于、缘于、在于、出自、起源于、来自、发源于、发自、源于、根源于、立足[于]\n",
    "        verb2_model:{Effect}<Prep...>{Cause}\n",
    "        '''\n",
    "\n",
    "        pattern = re.compile(r'(.*)(根源于|取决|来源于|出于|取决于|缘于|在于|出自|起源于|来自|发源于|发自|源于|根源于|立足|立足于)/[p|c]+\\s(.*)')\n",
    "        result = pattern.findall(sentence)\n",
    "        data = dict()\n",
    "        if result:\n",
    "            data['tag'] = result[0][1]\n",
    "            data['cause'] = result[0][2]\n",
    "            data['effect'] = result[0][0]\n",
    "        return data\n",
    "    '''9由果溯因居端式精确'''\n",
    "    def ruler9(self, sentence):\n",
    "        '''\n",
    "        cons3:因为、由于\n",
    "        cons3_model:{Effect}<Conj...>{Cause}\n",
    "        '''\n",
    "        pattern = re.compile(r'(.*)是?\\s(因为|由于)/[p|c]+\\s(.*)')\n",
    "        result = pattern.findall(sentence)\n",
    "        data = dict()\n",
    "        if result:\n",
    "            data['tag'] = result[0][1]\n",
    "            data['cause'] = result[0][2]\n",
    "            data['effect'] = result[0][0]\n",
    "\n",
    "        return data\n",
    "\n",
    "    '''抽取主函数'''\n",
    "    def extract_triples(self, sentence):\n",
    "        infos = list()\n",
    "      #  print(sentence)\n",
    "        if self.ruler1(sentence):\n",
    "            infos.append(self.ruler1(sentence))\n",
    "        elif self.ruler2(sentence):\n",
    "            infos.append(self.ruler2(sentence))\n",
    "        elif self.ruler3(sentence):\n",
    "            infos.append(self.ruler3(sentence))\n",
    "        elif self.ruler4(sentence):\n",
    "            infos.append(self.ruler4(sentence))\n",
    "        elif self.ruler5(sentence):\n",
    "            infos.append(self.ruler5(sentence))\n",
    "        elif self.ruler6(sentence):\n",
    "            infos.append(self.ruler6(sentence))\n",
    "        elif self.ruler7(sentence):\n",
    "            infos.append(self.ruler7(sentence))\n",
    "        elif self.ruler8(sentence):\n",
    "            infos.append(self.ruler8(sentence))\n",
    "        elif self.ruler9(sentence):\n",
    "            infos.append(self.ruler9(sentence))\n",
    "\n",
    "        return infos\n",
    "\n",
    "    '''抽取主控函数'''\n",
    "    def extract_main(self, content):\n",
    "        sentences = self.process_content(content)\n",
    "        datas = list()\n",
    "        for sentence in sentences:\n",
    "            subsents = self.fined_sentence(sentence)\n",
    "            subsents.append(sentence)\n",
    "            for sent in subsents:\n",
    "                sent = ' '.join([word.word + '/' + word.flag for word in pseg.cut(sent)])\n",
    "                result = self.extract_triples(sent)\n",
    "                if result:\n",
    "                    for data in result:\n",
    "                        if data['tag'] and data['cause'] and data['effect']:\n",
    "                            datas.append(data)\n",
    "        return datas\n",
    "\n",
    "    '''文章分句处理'''\n",
    "    def process_content(self, content):\n",
    "        return [sentence for sentence in SentenceSplitter.split(content) if sentence]\n",
    "\n",
    "    '''切分最小句'''\n",
    "    def fined_sentence(self, sentence):\n",
    "        return re.split(r'[？！，；]', sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.781 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "cause AmericanEagle四季度符合预期华尔街对其毛利率不满\n",
      "tag 导致\n",
      "effect 股价大跌\n",
      "************************\n",
      "cause AmericanEagle四季度符合预期华尔街对其毛利率不满\n",
      "tag 导致\n",
      "effect 股价大跌\n",
      "************************\n",
      "cause 我没有好好学习。\n",
      "tag 之所以-因为\n",
      "effect 考试没及格，是\n",
      "************************\n",
      "cause 天晴了，\n",
      "tag 因为-所以\n",
      "effect 我今天晒被子。\n",
      "************************\n",
      "cause 下雪了，\n",
      "tag 因为-所以\n",
      "effect 路上的行人很少。\n",
      "************************\n",
      "cause 早上没吃的缘故，\n",
      "tag 因为-所以\n",
      "effect 今天还没到放学我就饿了.\n",
      "************************\n",
      "cause 小华身体不舒服，\n",
      "tag 因为-所以\n",
      "effect 她没上课间操。\n",
      "************************\n",
      "cause 我昨晚没睡好，\n",
      "tag 因为-所以\n",
      "effect 今天感觉很疲倦。\n",
      "************************\n",
      "cause 李明学习刻苦，\n",
      "tag 因为-所以\n",
      "effect 其成绩一直很优秀。\n",
      "************************\n",
      "cause 它没有专一的目标，也不能持之以恒。\n",
      "tag 之所以-因为\n",
      "effect 不能把石块滴穿，是\n",
      "************************\n",
      "cause 他平时不努力学习。\n",
      "tag 之所以-因为\n",
      "effect 成绩不好，是\n",
      "************************\n",
      "cause 你没有学好关联词的用法。\n",
      "tag 之所以-因为\n",
      "effect 提这个问题，是\n",
      "************************\n",
      "cause 减了税\n",
      "tag 因此\n",
      "effect 怨声也少些了。\n",
      "************************\n",
      "cause 减了税\n",
      "tag 因此\n",
      "effect 怨声也少些了。\n",
      "************************\n",
      "cause 他的话引得大家都笑了\n",
      "tag 因此\n",
      "effect 轻松了很多。\n",
      "************************\n",
      "cause 他努力学习\n",
      "tag 因此\n",
      "effect 通过了考试。\n",
      "************************\n",
      "cause 明天要下雨\n",
      "tag 既然\n",
      "effect 就不要再出去玩。\n",
      "************************\n",
      "cause 他还是那么固执\n",
      "tag 既然\n",
      "effect 就不要过多的与他辩论。\n",
      "************************\n",
      "cause 别人的事与你无关\n",
      "tag 既然\n",
      "effect 你就不要再去过多的干涉。\n",
      "************************\n",
      "cause 梦想实现不了\n",
      "tag 既然\n",
      "effect 就换一个你自己喜欢的梦想吧。\n",
      "************************\n",
      "cause 别人需要你\n",
      "tag 既然\n",
      "effect 你就去尽力的帮助别人。\n",
      "************************\n",
      "cause 生命突显不出价值\n",
      "tag 既然\n",
      "effect 就去追求自己想要的生活吧。\n",
      "************************\n",
      "cause 别人不尊重你\n",
      "tag 既然\n",
      "effect 就不要尊重别人。\n",
      "************************\n",
      "cause 题目难做，就不要用太多的时间去想\n",
      "tag 既然\n",
      "effect 问一问他人也许会更好。\n",
      "************************\n",
      "cause 我们是学生\n",
      "tag 既然\n",
      "effect 就要遵守学生的基本规范。\n"
     ]
    }
   ],
   "source": [
    "'''测试'''\n",
    "\n",
    "content1 = \"\"\"\n",
    "截至2008年9月18日12时，5·12汶川地震共造成69227人死亡，374643人受伤，17923人失踪，是中华人民共和国成立以来破坏力最大的地震，也是唐山大地震后伤亡最严重的一次地震。\n",
    "\"\"\"\n",
    "content2 = '''\n",
    "2015年1月4日下午3时39分左右，贵州省遵义市习水县二郎乡遵赤高速二郎乡往仁怀市方向路段发生山体滑坡，发生规模约10万立方米,导致多辆车被埋，造成交通双向中断。此事故引起贵州省委、省政府的高度重视，省长陈敏尔作出指示，要求迅速组织开展救援工作，千方百计实施救援，减少人员伤亡和财物损失。遵义市立即启动应急救援预案，市应急办、公安、交通、卫生等救援力量赶赴现场救援。目前，灾害已造成3人遇难1人受伤，一辆轿车被埋。\n",
    "当地时间2010年1月12日16时53分，加勒比岛国海地发生里氏7.3级大地震。震中距首都太子港仅16公里，这个国家的心脏几成一片废墟，25万人在这场骇人的灾难中丧生。此次地震中的遇难者有联合国驻海地维和部队人员，其中包括8名中国维和人员。虽然国际社会在灾后纷纷向海地提供援助，但由于尸体处理不当导致饮用水源受到污染，灾民喝了受污染的水后引发霍乱，已致至少2500多人死亡。\n",
    "'''\n",
    "content3 = '''\n",
    "American Eagle 四季度符合预期 华尔街对其毛利率不满导致股价大跌\n",
    "我之所以考试没及格，是因为我没有好好学习。\n",
    "因为天晴了，所以我今天晒被子。\n",
    "因为下雪了，所以路上的行人很少。\n",
    "我没有去上课是因为我病了。\n",
    "因为早上没吃的缘故，所以今天还没到放学我就饿了.\n",
    "因为小华身体不舒服，所以她没上课间操。\n",
    "因为我昨晚没睡好，所以今天感觉很疲倦。\n",
    "因为李明学习刻苦，所以其成绩一直很优秀。\n",
    "雨水之所以不能把石块滴穿，是因为它没有专一的目标，也不能持之以恒。\n",
    "他之所以成绩不好，是因为他平时不努力学习。\n",
    "你之所以提这个问题，是因为你没有学好关联词的用法。\n",
    "减了税,因此怨声也少些了。\n",
    "他的话引得大家都笑了，室内的空气因此轻松了很多。\n",
    "他努力学习，因此通过了考试。\n",
    "既然明天要下雨，就不要再出去玩。\n",
    "既然他还是那么固执，就不要过多的与他辩论。\n",
    "既然别人的事与你无关，你就不要再去过多的干涉。\n",
    "既然梦想实现不了，就换一个你自己喜欢的梦想吧。\n",
    "既然别人需要你，你就去尽力的帮助别人。\n",
    "既然生命突显不出价值，就去追求自己想要的生活吧。\n",
    "既然别人不尊重你，就不要尊重别人。 因果复句造句\n",
    "既然题目难做，就不要用太多的时间去想，问一问他人也许会更好。\n",
    "既然我们是学生，就要遵守学生的基本规范。\n",
    "'''\n",
    "extractor = CausalityExractor()\n",
    "datas = extractor.extract_main(content3)\n",
    "for data in datas:\n",
    "    print('******'*4)\n",
    "    print('cause', ''.join([word.split('/')[0] for word in data['cause'].split(' ') if word.split('/')[0]]))\n",
    "    print('tag', data['tag'])\n",
    "    print('effect', ''.join([word.split('/')[0] for word in data['effect'].split(' ') if word.split('/')[0]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
